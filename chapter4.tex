\chapter{Implementation}
\label{cha:impl}
This chapter outlines the main stages in implementating the designed system in code.

% software environment to be mentioned in each section

The system to be developed, as initially described in Figure~\ref{fig:general}, is fairly representative of a Question-Answering (QA) system. QAs are data mining systems which use Information Retrieval (IR) and Information Extraction (IE) techniques to answer user queries. As such, this project was implemented in 3 stages, each corresponding to these subsystems in QA systems. The first of these was retrieving tweets, the IR phase of this project. Upon successful retrieval, information, the required features in this case, must be extracted and finally, these results must be displayed to the user in a simple, straightforward fashion. These stages will be further explored as follows:

\java
\section{Tweet Retrieval}
Without tweets, there is no work to be done, and so retrieving tweets can be regarded as the most important part of this project. The main objective of this stage is to retrieve as many relevant tweets from Twitter as possible. To do this, the system will interact with the set of public APIs Twitter provides in order to fulfill the requirements of each use case stated in Section~\ref{sec:uc}. The system is designed to use all of these to fulfill the requirements of each use case. This subsystem in the project is implemented in Java. This is because of its strong object oriented nature and platform independency. Of course, there are other options such as Python, however Java is a programming language with relatively straightforward multithreading capabilities.

\subsection{Streaming Twitter}
The Streaming API allows the system to fulfill the requirements of having a fully automated system that constantly monitors Twitter for software-related posts, as described by use case 1, in Section~\ref{sec:uc1}. 

The implementation at this stage utilises Twitter's filtering URL at \url{https://stream.twitter.com/1/statuses/filter.json} and passes it the set of dictionary terms and keywords to filter tweets by described in Section~\ref{sec:arc2}.

This implementation could have been done using the \emph{Twitter4J}\footnote{\url{http://twitter4j.org/}} Java library for Twitter integration, however most of the functions appear unnecessary and excessive in the scope of this project. For this reason, the Twitter Streaming API integration was implemented from the ground up.

Upon retrieving these filter terms from the database, the application formats this list into a string after which it creates three new Thread objects, a \emph{DatabaseThread} which will carry out all database operations, a \emph{StreamParseThread} which parses the stream of responses sent back from the Twitter server, and a \emph{ScannerThread}, which monitors the running state of the program, so as to allow a clean exit when the user wishes to quit. This scanner thread simply monitors the console input for users to type the exit command, upon which all connections are dropped and final parsing and database operations are carried out before closing the application. This high level control flow can be seen in Figure~\ref{fig:tweetir}.

On initialising these threads, the application attempts to set up a secure connection to Twitter using the HTTPS protocol. It uses the POST method to write the string of filter terms to the server in order to being receiving tweets. Once this connection is fully set up, Twitter will return JavaScript Object Notation (JSON) strings for each tweet, and so a JSON parser is set up using the Google-Gson Java library~\cite{gson}. The aforementioned threads are then started as the actual streaming process now begins.

For every tweet returned by the API, the application adds this JSON response, as a \emph{JsonObject}, to a queue in the \emph{StreamParseThread} class, using the following simple method:
\begin{lstlisting}[caption=Adding tweets to a parse queue, label=lst:queue]
private final List<JsonObject> parseList = new ArrayList<JsonObject>();

public boolean addTask(final JsonObject object) {
    synchronized (parseList) {
        return parseList.add(object);
    } // synchronized
} // addTask(JsonObject)
\end{lstlisting}

The parser thread now assumes control of the processing to be done, while the main thread continues to add to this \emph{parseList} queue. The parser thread has the sole task of parsing the information in this JSON object into a more meaningful \emph{Tweet} object. This class' properties can be seen in Listing~\ref{lst:tweetuser}. To do this, the JSON object first needs to be checked if it represents a tweet delete entity, that is, a object containing the ``delete'' key signifying a user has deleted their tweet. In such a case, Twitter requests that applications honour the user's requests and delete this tweet. If otherwise the JSON object is actually a tweet, the program extracts the Twitter user's details to check their locale. In cases where this is not English, a \texttt{null} value is returned and the tweet is ignored. If this test passes, all the remaining properties described in the \emph{Tweet} and \emph{User} classes are extracted and returned as a single Tweet object.

This Tweet object is encapsulated in an \emph{InsertKeywordTask} object. This class is an implementation of the \emph{DatabaseTask} interface, which is used to perform the different database operations when used in conjunction with the different \emph{DatabaseConnector} classes. The hierarchy of these classes can be seen in Figures~\ref{fig:dbtask} and \ref{fig:dbcon} respectively. To further clarify, the \emph{DatabaseThread} constructor takes a \emph{DatabaseConnector} object as an argument. This allows for a more extensible system as different types of database management systems can be used with the application. It must be noted that in the current implementation, the \emph{DatabaseThread} class has been implemented at a similarly high level of abstraction, and as such an \emph{SQLThread} extends this class for use with a MySQL database. The \emph{SQLThread} initialises with a \emph{TweetSQLConnector} object, as it only operates in classes related to tweet retrieval. The DatabaseThread class maintains its own queue of \emph{DatabaseTask} objects as the parser thread, previously seen in Listing~\ref{lst:queue}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=12cm]{tweetir}
\end{center}
\caption{Control flow for tweet retrieval subsystem}
\label{fig:tweetir}
\end{figure}

\begin{lstlisting}[caption=Tweet and User class properties, label=lst:tweetuser]
public class Tweet {
    private final long tweetId;
    private final String tweet;
    private final String createdAt;
    private final User user;
    private String keyword = null; // Only used when filtering
    ...
} // class Tweet

public final class User {
    private final long id;
    private final String username;
    ...
} // class User 
\end{lstlisting}

\begin{figure}[h]
\begin{center}
\includegraphics[width=7cm]{dbtask}
\end{center}
\caption{DatabaseTask class diagram}
\label{fig:dbtask}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=9cm]{dbcon}
\end{center}
\caption{DatabaseConnector class diagram}
\label{fig:dbcon}
\end{figure}

Class design aside, once this \emph{InsertKeywordTask} object has been created, it is added to the queue in the \emph{DatabaseThread}. The respective implementations of the \emph{doTask()} method in each of the DatabaseTask classes will be performed as this queue is emptied. In the case of InsertKeywordTask, this is simply \texttt{db.insertTweet(t)}, where \texttt{db} is the TweetDatabaseConnector passed to it in the \texttt{doTask()} method, and \texttt{t} is the Tweet object it was initialised with.

This process is completed when the \emph{TweetDatabaseConnector} inserts the tweet into the database, in the current implementation using Java Database Connectivity (JDBC) to manipulate the MySQL server.

\subsection{Searching Twitter}
\label{sec:searchjava}
The Search API will now be used to handle use case 2 of the designed system, as described in Section~\ref{sec:uc2}. With the Search API not operating in real time, the realisation of this use case can afford to be a much simpler system. The levels of multithreading displayed in streaming Twitter will not be required, as interaction with the API is more of a serial communication as can be seen in Figure~\ref{fig:ucsearch}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=13cm]{ucsearch}
\end{center}
\caption{Searching Twitter sequence diagram}
\label{fig:ucsearch}
\end{figure}

The implementation of the Twitter search use case utilises the Twitter Search API URL at \url{http://search.twitter.com/search.json}. The API also offers eXtensible Markup Language (XML) format responses, however, working with JSON allows consistency within the system. As with the implementation of the Twitter stream use case, the application begins with the initialision of a \emph{DatabaseThread} and is initially given a single keyword which will be searched for. This keyword is chosen by the end user. A simple HTTP GET request is then made to the above URL and Twitter then returns up to 1500 tweets from the last seven days corresponding to the search term.

The Search API returns a different JSON response to that of the Streaming API. Each JSON string contains an \texttt{iso\_language\_code}, and this will be used to filter tweets by language. Once the desired information has been extracted, i.e. the properties of the \emph{Tweet} class, the remaining operations are carried out just as they are in the realisation of the Twitter streaming use case, that is, the tweet is encapsulated in an \emph{InsertKeywordTask} object and this is added to a queue in the \emph{DatabaseThread} for the insert task to be carried out.

% SHOW JSON RESPONSE IN APPENDIX


\python
\section{Feature Extraction}
\label{sec:fe}
Once tweets have been retrieved from Twitter and stored in the MySQL database, they are now available for feature extaction, which can be regarded as the core stage in implementing the system. This subsystem involves using NLP techniques to extract the information described in Section~\ref{sec:arc2} from each tweet. This subsystem is implemented in Python 2.7 due to the raw power it possesses and also due to the decision to use the Natural Language Toolkit (NLTK). It was felt that Python's speed and text manipulation would allow for a better implementation of the system.

There are many steps involved in implementing the feature extraction. These are explored in order of execution.

\subsection{Sentiment Analysis}
Sentiment analysis was recognised as one of the key features to be extracted from the initial design stages (see Figure~\ref{fig:phase2}). It has been implemented using the Sentiment140 Bulk Classification Service API. 
The sentiment analysis of tweets is carried out before any of the other feature extraction work. As tweets have been retrieved and stored in the MySQL database, this part of the system selects the latest tweets, retrieving just the id and text, that have yet to be analysed and packs them into a JSON string object of the format:

\begin{verbatim}
{ "data": [ { "id": "1234", "text": "Google Chrome is awesome!" },
            { "id": "1235", "text": "Safari 5.0.2 is out now" },
            { "id": "1236", "text": "I really hate the new Firefox" } ] }
\end{verbatim}

This JSON string is then posted to the Twitter Sentiment API where classifications into the positive, negative and neutral classes are carried out by a Maximum Entropy classifier, using unigram and bigram features, trained with tweets containing emoticons. The internal specifics of a Maximum Entropy classifier, however, is not in the scope of this project.

Currently only 100 tweets are analysed at a time due to time constraints when users wish to run the program in real time. By using a small number, the data needing to be transferred is minimal and allows for a more interactive user experience.

Using the previous example, the data is returned by the server in the following format, with a polarity field added to each analysed tweet with values 0, 2 and 4 corresponding to negative, neutral and positive respectively.
\begin{verbatim}
{ "data": [ 
  { "id": "1234", "text": "Google Chrome is awesome!", "polarity": 4},
  { "id": "1235", "text": "Safari 5.0.2 is out now", "polarity": 2 },
  { "id": "1236", "text": "I really hate the new Firefox", "polarity": 0 } 
] }
\end{verbatim}

Upon receipt of this response, the JSON formatted string is parsed and the corresponding record for the tweet previously stored in the MySQL database is updated with new values for sentiment score and the actual sentiment, using polarity and its semantic meaning respectively.

\subsection{URL Extraction}
Before extracting context and semantics from tweets, any URLs mentioned are found and removed. Assuming the tweet is software-related, these URLs are quite likely to be links to the software, or further reviews. This task is done using NLTK's \texttt{regexp\_tokenize()} function with \url{http://}\verb/[^ ]+/ passed as the regular expression that finds URLs. If the tweet is later found not to contain any software, these URLs are discarded. Potential issues with this implementation could be that a user may post a URL without the preceding \url{http://} protocol prefix and these would not be found by this regular expression. However, Twitter automatically converts URLs to their \url{http://t.co/} domain and so this is resolved on the Twitter server side.

\subsection{Tokenisation}
After URLs have been extracted and removed from the source text, the tweet is tokenised to produce an array of all the terms in the tweet. The tokenisation process is also done using NLTK's \texttt{regexp\_tokenize()} function, passing it the regular expression - \verb/\w+([.,]\w+)*|\S+/. This expression returned superior results to alternation tokenisation functions provided by NLTK, such as \texttt{wordpunct\_tokenize()} as it was capable of finding numbers and currencies without splitting them. Using the above example,

\begin{quote}
I really hate the new Firefox
\end{quote}
this would be tokenised to the following:
\begin{quote}
[`I', `really', `hate', `the', `new', `Firefox']
\end{quote}

The following example shows a more complicated tokenisation process.
\begin{quote}
Norton Anti-Virus released for \$50 \#ripoff

\begin{math}\Rightarrow\end{math}
[`Norton', `Anti', `-Virus', `released', `for', `\$50', `\#ripoff']
\end{quote}

\subsection{Price Extraction}
\label{sec:price}
Continuing on from this tokenisation of the original source text, the current subsystem attempts to find prices in the array of terms. This is done using Python's built-in regular expression module, \texttt{re}. A number of regular expressions are used to define patterns denoting numbers, currencies and quantifiers like `hundred' and `thousand'. As the form of prices vary, for example in the case of mobile apps you might find `£0.59', `59p' or even `59 pence', these combinations of tokens may be split across two tokens in the array returned from the tokenisation process. For this reason, it is necessary to iterate over all items in the list of tokens while remembering the previous one. This obviously means a less efficient system, however, it has produced the best results in such variable conditions.

\subsection{Part-of-speech (POS) Tagging}
The POS tagger used by this system is taken from the NLTK modules and uses the \texttt{pos\_tag()} function which takes a tokenised sentence as its only argument. Continuing from the first example, this process tags as follows: 
\begin{quote}
[`I', `really', `hate', `the', `new', `Firefox']

\begin{math}\Rightarrow\end{math}
[(`I', `PRP'), (`really', `RB'), (`hate', `JJ'), (`the', `DT'), (`new', `JJ'), (`Firefox', `NNP')]

\begin{center}
\begin{tabular}{ l | l }
  \hline                        
  PRP & Pronoun \\
  RB & Adverb \\
  JJ & Adjective \\
  DT & Determiner \\
  NNP & Proper Noun \\
  \hline  
\end{tabular}
\end{center}
\end{quote}

\subsection{N-Grams}
N-grams are sequences of n tokens from a given source text. The implementation of creating n-grams in this project is done using the \texttt{nltk.util.ngrams()} function. This process starts by creating a five-gram of the tweet tokens. This means a sequence of five tokens will be created from the array of tokens. The system utilises a five-gram sequence due to potentially long software names, basing this on the na\"{\i}ve assumption that these names will not exceed five words. This will allow for improved extraction of software names in the next stage. Using the Firefox tweet as a running example, the outcome of this five-gram modelling process can be seen below.

\begin{quote}
[`I', `really', `hate', `the', `new', `Firefox']

\begin{math}\Rightarrow\end{math}

[
 ( (`I', `PRP'), (`really', `RB'), (`hate', `JJ'), (`the', `DT'), (`new', `JJ') ),

 ( (`really', `RB'), (`hate', `JJ'), (`the', `DT'), (`new', `JJ'), (`Firefox', `NNP') )
]
\end{quote}

\subsection{Main Feature Extraction}
This tagging process consists of the core functions of the proposed system. Its purpose is to extract all the features that have yet to be extracted, that is, software names and versions, companies, programming languages and operating systems. It also attempts to find any prices that may previously have been missed, and also has the task of discovering software that is not already in the dictionary.

Having created a set of five-gram sequences from the tweet, the application may now iterate through each of these in an attempt to find any information that has not yet been found. For each of these sequences, the program iterates through each POS-tagged token in the sequence. The tagging process then proceeds as follows: 
\newline
\begin{algorithmic}
\If {token is tagged as a noun}
    \If {token is in dictionary of software, companies, os, programming languages}
        \If {previous token not tagged as determiner or preposition}
            \State Feature has been found
        \EndIf
    \EndIf
\EndIf
\end{algorithmic}

This rule filters out linguistics of the form shown in Figure~\ref{fig:rule1}.
\begin{figure}[h!]
 \centering
  \input{lingrule1}
  \caption{A linguistic filter
    \label{fig:rule1}}
\end{figure}
If however, these conditions fail, usually in the case where none of the tokens are in the dictionary of keywords used to retrieve these tweets, a regular expression is used to find clues to the presence of new software.

\begin{quote}
\verb~^download$|^get$~
\end{quote}

The above regular expression matches on the words \emph{download} or \emph{get}. This works on the basis that many tweets about software are usually posted to promote said software. This is generally done by urging others to download it, and that too by means of application stores like the App Store, or Google Play. This then allows the next token to be analysed to check if it is in fact a piece of software. This is done by checking that token's part of speech tag, and if it is a noun, the following tokens are also assessed in case the name of the software is longer than one word. This possible software name is then noted and kept aside for verification by web search as discussed in Section~\ref{sec:bing}. This regular expression can be applied to the five-gram in conjunction with others in order to maximise the number of features extracted. The following expression could be used to find an operating system.

\begin{quote}
\verb~^on$|^for$~
\end{quote}

By applying these together in the form displayed in Figure~\ref{fig:rule2}, the system may be able to determine the platform upon which a piece of software runs.

\begin{figure}[h!]
 \centering
  \input{lingrule2}
  \caption{A linguistic pattern to find software and the operating system it may run on
    \label{fig:rule2}}
\end{figure}

Once software has been found in the tweet, a search for its version number begins. This essentially works on the assumption that once software has been named, if a version number is to appear, it will be stated either immediately afterwards, or after the word \emph{version}, or some derivative, such as \emph{ver} or \emph{v}.

Finally, this section of the implementation does one more check for prices, this is mainly for free software, as the word \emph{free} would not be found at the price extraction stage explored in Section~\ref{sec:price}. 
\begin{quote}
\verb~^is$|^for$~
\newline
\verb~^free$~
\end{quote}

The above regular expressions are used similarly to the usage of the expression finding operating systems that software may run on. If the first expression is matched in the text, it is likely the next token is a price. In cases where the next token is \emph{free}, it will match the second regular expression, thus signifying the price of the software. This is a na\"{i}ve approach but in most cases the prices will already have been extracted.

This concludes the main feature extraction process leaving just the verification of new software names via a web search. Listings~\ref{lst:extr} and \ref{lst:extr2} show examples of the data structure storing these extracted features.

\begin{lstlisting}[caption=Example of some extracted features, label=lst:extr]
{
  "company_id" : "23",
  "company_name" : "apple",
  "os_id" : "9",
  "os_name" : "ios",
  "software_name" : "ios",
  "tweet" : "There are some new wallpaper in Apple iOS 5.1. Check them out!",
  "tweet_db_id" : "439640",
  "version" : "5.1",
  "sentiment": "positive"
}
\end{lstlisting}

\begin{lstlisting}[caption=Another example of some extracted features, label=lst:extr2]
{
  "price" : "free",
  "software_id" : "159",
  "software_name" : "moodle",
  "tweet" : "Training hundreds or thousands of staff?? If you want easy, affordable e-learning training then download moodle free http://t.co/beZgqaOd",
  "tweet_db_id" : "440065",
  "url" : "http://t.co/beZgqaOd"
}
\end{lstlisting}

\subsection{Software Verification}
\label{sec:bing}
The feature extraction subsystem may discover new software, and as such needs to verify these are actually pieces of software and not something else. To do this the program utilises the Microsoft Bing API which returns web search queries. As the main tagging process checks the dictionary for matching software names, and the tweet retrieval engine uses both the dictionary and a set of keywords, there will be some pieces of software mentioned in the tweets that are not in the dictionary. As a result, these will be flagged as possible software names, and then queried on the Microsoft Bing search engine with the keywords ``movie'', ``music'', and ``software game''. These keywords were selected on the basis that the initial search key terms retrieved many tweets referring to music and films. The implementation of the Bing API integration can be seen in Appendix~\ref{app:bing}.
\newline
\begin{algorithmic}
\Function {bing\_search}{bing, term}
    \State music = bing.search(term, music)
    \State movie = bing.search(term, movie)
    \State software = bing.search(term, software game)
    \newline
    \If {size(software) greater than size(movie) and size(music)}
        \If {references to software in title and description}
            \State \Return True
        \EndIf
    \EndIf
    \newline
    \State \Return False
\EndFunction
\end{algorithmic}
If the number of results for software associated with the searched term is greater than corresponding results for films and music, the results are checked for identifiers of software in their headings. Therefore if any of the results suggests the searched term is a piece of software, that is assumed true.

\section{Storing the Extracted Information}
As the information being extracted is temporarily stored in a Python dictionary variable, it is essentially in the form of a JSON string. The database design for storing this information is also in the form of a NoSQL database. For this reason, a document-based database system seems to be the best approach. By using MongoDB, it is easy to store the extracted information, as it is as straightforward as directly storing the string representation of this variable as a record in the database. Once this information has been successfully stored in MongoDB, the boolean \emph{tagged} flag is set to True for the corresponding tweet in the MySQL database.

\section[Visualisation]{Visualisation/Graphical User Interface(GUI)}
The final stage of the project is to aggregate and present the results to the user in a GUI.

\subsection{Aggregation}
The aggregation process has been implemented in Python because the MongoDB wrapper class has already been written and due to the simple mapping between Python dict types and MongoDB's stored documents.

This stage involes two processes. The first of these is finding the general sentiment towards a particular piece of software from all of the tweets mentioning said software, as in use case 3 (Section~\ref{sec:uc3}). This is a simple \emph{find} query and the command and syntax for carrying out this task in Python is equivalent to its MongoDB counterpart. The following command would retrieve documents containing the key-value pair of \texttt{software\_name=firefox}, given the collection, MongoDB's equivalent of a table in a relational database, is named \texttt{tagged\_tweets}.
\begin{quote} 
\begin{lstlisting}
db.tagged_tweets.find( { `software_name': `firefox' } )
\end{lstlisting}
\end{quote}

Upon retrieving these documents, the application extracts the distinct values from the \texttt{sentiment} field of each document, which can be \emph{positive}, \emph{negative} or \emph{neutral}. For each of these values that appears, its appearance count is calculated using a looping function and these counts are used to make charts in the web application, the topic of discussion in Section~\ref{sec:webapp}.

The second task, use case 4 (Section~\ref{sec:uc4}), is to find the top tweeted software and also includes operating systems in the final implementation. This is completed using a grouping function defined in the Python MongoDB driver module.

\begin{quote}
\begin{lstlisting}
def _group(self, key):
        return self.tags.group(key=[key],
                               condition={key:{`\$ne':None}},
                               initial={`count':0},
                               reduce=`function(obj,prev) { prev.count += 1;}')
\end{lstlisting}
\end{quote}

The above function returns a list of items grouped by a given \texttt{key}. The items in the list are also given a \texttt{count} value which is the number of times that software name will have appeared in the database collection. This list is then sorted in descending order and a slice of the top ten of these results are returned to be displayed to the user.

\subsection{Web Application}
\label{sec:webapp}
The web application is required to work with both the Python and the Java code in this project. As such, a static web page is far from the desired solution. This leaves the options of a Java web servlet or a Python web application. Both of these are good options, however, running a Python interpreter inside the Java Virtual Machine(JVM) is a much slower process than invoking Java classes from Python. This can be done using Jython, a Python implementation for Java that allows developers to both invoke Java from Python and vice versa \cite{Juneau:2010}. An alternative solution is to run shell commands directly through the Python interpreter in order to run the Java classes. This is the chosen route of action because no information needs to be passed between the two platforms that cannot be sent as command-line arguments when running the Java code.

Now that this has been decided, a Python web framework must be chosen for the development of this GUI. The CherryPy web framework was selected ahead of the likes of Django and Pylons due to its simplicity and pythonic interface \cite{cherrypy}. For an appealing web design that is easy to create, a templating language must be used to embed Python code. This will be done using the Mako template library\footnote{\url{http://www.makotemplates.org/}}. Mako is a leading templating library that is used by some major websites, such as \url{python.org} itself. Mako was chosen ahead of others such as Genshi, which is an XML-based templating engine, as its syntax is much more similar to that of normal Python code. Mako templates also support inheritance as can be seen in Listing~\ref{lst:mako} provided in Appendix~\ref{app:webapp}. HTML5, CSS and the JavaScript library jQuery make up the rest of the web development environment.

As designed, the web application has to carry out three functions. Users must be able to \textbf{search} Twitter, \textbf{analyse} stored results and \textbf{view} the top tweeted operating systems and software. The first of these tasks, the search, is carried out using the Java implementation of integrating Twitter's Search API. Python's \texttt{subprocess} module allows shell commands to be executed inside the Python interpreter. This feature is exploited to run the \\\texttt{uk.ac.manchester.cs.patelt9.twitter.SearchAPI} class, its implementation details explored in Section~\ref{sec:searchjava}, and check its output in the shell. This output is piped through to the web application and finally displayed on the web page. This is shown in Chapter~\ref{cha:results} when discussing the results of the project.

The user may then opt to follow through the entire feature extraction process. By restricting the search to a fairly small number of tweets, this process is heavy in terms of time consumption. By opting to do so, the user allows the application to check the sentiment of each of the tweets, and then follow through the feature extraction process described in Section~\ref{sec:fe}. At the end of this extraction, all the new information is returned in the form of a list, ready for display on the web page.

The next task is to show the user the results that have already been stored in the database. As per the design, this is done by showing the user a drop down menu consisting of all of the software that exists in the database. Upon selecting from the list, the application uses the aggregation methods described in the previous section to retrieve sentiment data. These are then used as input to the Google Chart Tools API, which creates pie charts displaying percentages for each distinct sentiment value.

Finally, the web application's home page shows a list of the top ten most tweet operating systems and software as links to the charts of these tools. This is again done using the aggregation methods set up in the previous stage of implementation.

\section{Testing}

\section{Summary}

